# === Secrets ===
# Für Produktion: ./bin/generate-secrets.sh ausführen (Docker Compose Secrets)
# Für Entwicklung: Werte hier direkt setzen

# Datenbank
# POSTGRES_PASSWORD=changeme  (Produktion: secrets/postgres_password)

# Home Assistant
HOME_ASSISTANT_URL=http://homeassistant.local:8123
# HOME_ASSISTANT_TOKEN=your_token  (Produktion: secrets/home_assistant_token)

# n8n (Workflow Automation via MCP)
# N8N_MCP_ENABLED=false
# N8N_BASE_URL=http://n8n.local:5678      # Used in Pydantic Settings
# N8N_API_URL=http://n8n.local:5678       # Used by n8n-mcp stdio subprocess
# N8N_API_KEY=your_n8n_api_key            # n8n → Settings → API → Create API Key

# Calendar (Unified Calendar MCP Server — EWS, Google, CalDAV)
# CALENDAR_ENABLED=false
# CALENDAR_CONFIG=/config/calendar_accounts.yaml
# Exchange (EWS): CALENDAR_WORK_USERNAME, CALENDAR_WORK_PASSWORD
# Google Calendar: credentials.json + initial OAuth (see docs/kalender-integration-plan.md)
# Nextcloud CalDAV: CALENDAR_VEREIN_USERNAME, CALENDAR_VEREIN_PASSWORD

# Email MCP (IMAP/SMTP via renfield-mcp-mail)
# EMAIL_MCP_ENABLED=false
# MAIL_ACCOUNTS_CONFIG=/config/mail_accounts.yaml
# MAIL_REGFISH_PASSWORD=                  # (Produktion: secrets/mail_regfish_password)

# Frigate (Kamera-System)
FRIGATE_URL=http://frigate.local:5000

# Ollama LLM
# URL zur Ollama-Instanz (Standard: http://ollama:11434 für Docker-Container)
# Für externe GPU-Instanz z.B.: http://cuda.local:11434
OLLAMA_URL=http://ollama:11434

# Optional: Fallback-Ollama wenn OLLAMA_URL nicht erreichbar (z.B. GPU-Host offline)
# Empfohlen wenn OLLAMA_URL auf ein externes Gerät zeigt (cuda.local etc.)
# Im Container: http://host.docker.internal:11434 = Ollama auf dem Docker-Host
# OLLAMA_FALLBACK_URL=http://host.docker.internal:11434

# Timeout-Konfiguration (Standard: connect=10s, read=300s)
# OLLAMA_CONNECT_TIMEOUT=10.0
# OLLAMA_READ_TIMEOUT=300.0

# Multi-Modell Konfiguration (see docs/LLM_MODEL_GUIDE.md for recommendations)
OLLAMA_CHAT_MODEL=qwen3:14b        # Für normale Konversation (empfohlen: qwen3:14b)
OLLAMA_RAG_MODEL=qwen3:14b         # Für RAG-Antworten (empfohlen: qwen3:14b)
OLLAMA_EMBED_MODEL=qwen3-embedding:4b # Für Embeddings (768 Dimensionen, empfohlen: qwen3-embedding:4b)
OLLAMA_INTENT_MODEL=qwen3:8b       # Für Intent-Erkennung (empfohlen: qwen3:8b)

# Legacy (wird als OLLAMA_CHAT_MODEL verwendet falls gesetzt)
OLLAMA_MODEL=qwen3:8b

# Sprache
DEFAULT_LANGUAGE=de
WHISPER_MODEL=base
PIPER_VOICE=de_DE-thorsten-high

# Optional: OpenAI API für Fallback (falls offline nicht ausreicht)
# OPENAI_API_KEY=sk-...

# RAG (Retrieval-Augmented Generation)
RAG_ENABLED=true
RAG_CHUNK_SIZE=512              # Token-Limit pro Chunk
RAG_CHUNK_OVERLAP=50            # Überlappung zwischen Chunks
RAG_TOP_K=5                     # Anzahl der relevantesten Chunks pro Anfrage
RAG_SIMILARITY_THRESHOLD=0.7    # Minimum Similarity (0-1)

# Document Upload
UPLOAD_DIR=/app/data/uploads
MAX_FILE_SIZE_MB=50
ALLOWED_EXTENSIONS=pdf,docx,doc,txt,md,html,pptx,xlsx

# Agent Loop (ReAct — Multi-Step Tool Chaining)
AGENT_ENABLED=false
# AGENT_MAX_STEPS=5
# AGENT_STEP_TIMEOUT=30.0
# AGENT_TOTAL_TIMEOUT=120.0
# AGENT_MODEL=                       # Optional: eigenes Modell für Agent

# MCP Client (Model Context Protocol — externe Tool-Server)
MCP_ENABLED=false
# MCP_CONFIG_PATH=config/mcp_servers.yaml
# MCP_REFRESH_INTERVAL=60
# MCP_CONNECT_TIMEOUT=10.0
# MCP_CALL_TIMEOUT=30.0

# MCP Server-spezifische Variablen (Beispiele)
# HA_MCP_ENABLED=false
# HA_MCP_URL=http://homeassistant.local:8123/api/mcp

# Conversation Memory (Langzeitgedaechtnis)
# MEMORY_ENABLED=false                       # Langzeitgedaechtnis aktivieren (opt-in)
# MEMORY_EXTRACTION_ENABLED=false            # Fakten automatisch aus Dialogen extrahieren

# Proaktive Benachrichtigungen (Webhooks von HA-Automationen)
# PROACTIVE_ENABLED=false              # Master-Switch (opt-in)
# PROACTIVE_SUPPRESSION_WINDOW=60      # Dedup-Fenster in Sekunden
# PROACTIVE_TTS_DEFAULT=true           # TTS standardmäßig an
# PROACTIVE_NOTIFICATION_TTL=86400     # Ablauf in Sekunden (24h)

# Phase 2: Notification Intelligence (alle opt-in)
# PROACTIVE_SEMANTIC_DEDUP_ENABLED=false       # Semantische Deduplizierung via pgvector
# PROACTIVE_SEMANTIC_DEDUP_THRESHOLD=0.85      # Cosine Similarity Schwelle
# PROACTIVE_URGENCY_AUTO_ENABLED=false         # LLM-basierte Urgency-Klassifizierung
# PROACTIVE_ENRICHMENT_ENABLED=false           # LLM Content Enrichment
# PROACTIVE_ENRICHMENT_MODEL=                  # Optionales separates Modell
# PROACTIVE_FEEDBACK_LEARNING_ENABLED=false    # Feedback-basierte Unterdrückung
# PROACTIVE_FEEDBACK_SIMILARITY_THRESHOLD=0.80 # Schwelle für semantisches Matching

# MCP Notification Polling (opt-in) — polls MCP servers for proactive notifications
# NOTIFICATION_POLLER_ENABLED=false             # Master switch for MCP notification polling
# NOTIFICATION_POLLER_STARTUP_DELAY=30          # Delay before first poll (seconds)

# Reminders (opt-in)
# PROACTIVE_REMINDERS_ENABLED=false            # Timer-Erinnerungen
# PROACTIVE_REMINDER_CHECK_INTERVAL=15         # Prüfintervall in Sekunden

# Scheduling (Cron-basiert): Wird extern via n8n-Workflows oder HA-Automationen gelöst.
# Diese senden per Webhook an POST /api/notifications/webhook.
# Siehe: docs/PROACTIVE_SCHEDULING_TEMPLATES.md

# Monitoring
# METRICS_ENABLED=false                # Prometheus /metrics Endpoint (opt-in)

# Logging
LOG_LEVEL=INFO
